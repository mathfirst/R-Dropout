{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f61cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## R-dropout instantiation for the fashion mnist dataset\n",
    "### This code is adapted from an official Pytorch tutorial, see https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "batchsize = 32\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batchsize)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa462d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an NLP model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2084b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "kl_loss_fn = nn.KLDivLoss(reduction=\"sum\", log_target=True)\n",
    "nll_loss_fn = nn.NLLLoss()\n",
    "def r_dropout_loss(model, data, y, alpha=0.05): \n",
    "    # alpha is the regularization coefficient. It should be not too large.\n",
    "    batch_size = data.size(0)\n",
    "    data = torch.cat([data,data], dim=0)\n",
    "    pred = model(data)\n",
    "    log_probs = log_softmax(pred)\n",
    "    log_probs1, log_probs2 = log_probs[:batch_size, :], log_probs[batch_size:, :]\n",
    "    nll_loss = 0.5*( nll_loss_fn(log_probs1, y) + nll_loss_fn(log_probs2, y) ) # (nll_loss1+nll_loss_2)/2\n",
    "    kl_loss = 0.5*( kl_loss_fn(log_probs1, log_probs2) + kl_loss_fn(log_probs2, log_probs1) ) # (KL(p||q)+KL(q||p))/2\n",
    "    loss = nll_loss + alpha*kl_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae6d7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop and test loop\n",
    "def train_loop(dataloader, model, nll_loss_fn, optimizer, R_Dropout=False, alpha=.05):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss  \n",
    "        if R_Dropout:\n",
    "            loss = r_dropout_loss(model, X, y, alpha)\n",
    "        else:\n",
    "            pred = model(X)\n",
    "            loss = nll_loss_fn(log_softmax(pred), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 400 == 0:\n",
    "            loss, current = loss.item(), batch * len(y)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, nll_loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            log_probs = log_softmax(pred)\n",
    "            test_loss += nll_loss_fn(log_probs, y).item()\n",
    "            correct += (log_probs.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca72899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.261373  [    0/60000]\n",
      "loss: 0.553106  [12800/60000]\n",
      "loss: 0.592600  [25600/60000]\n",
      "loss: 0.445376  [38400/60000]\n",
      "loss: 0.397838  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.512097 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.421200  [    0/60000]\n",
      "loss: 0.267619  [12800/60000]\n",
      "loss: 0.521208  [25600/60000]\n",
      "loss: 0.458661  [38400/60000]\n",
      "loss: 0.522558  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.504001 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.373268  [    0/60000]\n",
      "loss: 0.423727  [12800/60000]\n",
      "loss: 0.623614  [25600/60000]\n",
      "loss: 0.318316  [38400/60000]\n",
      "loss: 0.515666  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.467599 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.313050  [    0/60000]\n",
      "loss: 0.325350  [12800/60000]\n",
      "loss: 0.395733  [25600/60000]\n",
      "loss: 0.434972  [38400/60000]\n",
      "loss: 0.476629  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.481612 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.252219  [    0/60000]\n",
      "loss: 0.398969  [12800/60000]\n",
      "loss: 0.396822  [25600/60000]\n",
      "loss: 0.581057  [38400/60000]\n",
      "loss: 0.284057  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.473503 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.289422  [    0/60000]\n",
      "loss: 0.295197  [12800/60000]\n",
      "loss: 0.632737  [25600/60000]\n",
      "loss: 0.302434  [38400/60000]\n",
      "loss: 0.319118  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.460786 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.233813  [    0/60000]\n",
      "loss: 0.333604  [12800/60000]\n",
      "loss: 0.396136  [25600/60000]\n",
      "loss: 0.442033  [38400/60000]\n",
      "loss: 0.388052  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.469842 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.343105  [    0/60000]\n",
      "loss: 0.333320  [12800/60000]\n",
      "loss: 0.354529  [25600/60000]\n",
      "loss: 0.326950  [38400/60000]\n",
      "loss: 0.390870  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.463067 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.249193  [    0/60000]\n",
      "loss: 0.309436  [12800/60000]\n",
      "loss: 0.313700  [25600/60000]\n",
      "loss: 0.312736  [38400/60000]\n",
      "loss: 0.362447  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.454037 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.373479  [    0/60000]\n",
      "loss: 0.371671  [12800/60000]\n",
      "loss: 0.521881  [25600/60000]\n",
      "loss: 0.299937  [38400/60000]\n",
      "loss: 0.330602  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.452392 \n",
      "\n",
      "Training without R Dropout is done! Training with R Dropout begins.\n"
     ]
    }
   ],
   "source": [
    "# training without R Dropout\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,R_Dropout=False)\n",
    "    accuracy = test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Training without R Dropout is done! Training with R Dropout begins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e6ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.312427  [    0/60000]\n",
      "loss: 0.546848  [12800/60000]\n",
      "loss: 0.843776  [25600/60000]\n",
      "loss: 0.655903  [38400/60000]\n",
      "loss: 0.589552  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.505939 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.564342  [    0/60000]\n",
      "loss: 0.461426  [12800/60000]\n",
      "loss: 0.587409  [25600/60000]\n",
      "loss: 0.479830  [38400/60000]\n",
      "loss: 0.432772  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.477541 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.485433  [    0/60000]\n",
      "loss: 0.480954  [12800/60000]\n",
      "loss: 0.439569  [25600/60000]\n",
      "loss: 0.469620  [38400/60000]\n",
      "loss: 0.434831  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.463533 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.395898  [    0/60000]\n",
      "loss: 0.346613  [12800/60000]\n",
      "loss: 0.500517  [25600/60000]\n",
      "loss: 0.597435  [38400/60000]\n",
      "loss: 0.461014  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.444258 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.394825  [    0/60000]\n",
      "loss: 0.315682  [12800/60000]\n",
      "loss: 0.471876  [25600/60000]\n",
      "loss: 0.574014  [38400/60000]\n",
      "loss: 0.571185  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.444992 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.432681  [    0/60000]\n",
      "loss: 0.438246  [12800/60000]\n",
      "loss: 0.595801  [25600/60000]\n",
      "loss: 0.582502  [38400/60000]\n",
      "loss: 0.423898  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.424903 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.430754  [    0/60000]\n",
      "loss: 0.392289  [12800/60000]\n",
      "loss: 0.593445  [25600/60000]\n",
      "loss: 0.378845  [38400/60000]\n",
      "loss: 0.458920  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.433463 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.375506  [    0/60000]\n",
      "loss: 0.397724  [12800/60000]\n",
      "loss: 0.452891  [25600/60000]\n",
      "loss: 0.549533  [38400/60000]\n",
      "loss: 0.405355  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.427740 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.357888  [    0/60000]\n",
      "loss: 0.391506  [12800/60000]\n",
      "loss: 0.469611  [25600/60000]\n",
      "loss: 0.480269  [38400/60000]\n",
      "loss: 0.550034  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.428135 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.369887  [    0/60000]\n",
      "loss: 0.359946  [12800/60000]\n",
      "loss: 0.639397  [25600/60000]\n",
      "loss: 0.406227  [38400/60000]\n",
      "loss: 0.414909  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.415180 \n",
      "\n",
      "Training with R Dropout is done!\n",
      "Regular training model gives an accuracy of 84.26%\n",
      "R Dropout training model gives an accuracy of 85.34%\n"
     ]
    }
   ],
   "source": [
    "# training with R Dropout\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, nll_loss_fn, optimizer,R_Dropout=True, alpha=.05)\n",
    "    accuracy_R_Dropout = test_loop(test_dataloader, model, nll_loss_fn)\n",
    "print(\"Training with R Dropout is done!\")\n",
    "print(f\"Regular training model gives an accuracy of {(100*accuracy):>0.2f}%\")\n",
    "print(f\"R Dropout training model gives an accuracy of {(100*accuracy_R_Dropout):>0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eff626",
   "metadata": {},
   "source": [
    "R-Dropout indeed improves the performance of the model a little. You can check out the above code if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af98b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
